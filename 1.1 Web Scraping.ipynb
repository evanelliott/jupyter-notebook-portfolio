{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup\n",
    "\n",
    "This web scraper collects roughly 300 of the latest news articles and metadata from the website \"Hacker News\" about topics such as cyber attacks, computer security, and entrepreneurship.\n",
    "\n",
    "Python libraries:\n",
    "- *requests* : gets webpages from the internet;\n",
    "- *BeautifulSoup* : converts a webpage from HTML to a searchable object;\n",
    "- *pandas* : provides a tabular data structure which will contain scraped data;\n",
    "- *re* : regular expressions provides ways to match and substitute sub-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading website home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://thehackernews.com/\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"#395697\" name=\"theme-color\"/>\n",
      "  <link as=\"style\" href=\"/css/roboto.css\" rel=\"preload\"/>\n",
      "  <link href=\"/css/roboto.css\" media=\"print\" onload=\"this.media='all\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.prettify()[:250] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering article URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL link to each article is found in the body of the home page, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://thehackernews.com/2020/11/become-white-hat-hacker-get-10-top.html',\n",
       " 'https://thehackernews.com/2020/11/interpol-arrest-3-nigerian-bec-scammers.html',\n",
       " 'https://thehackernews.com/2020/11/2-factor-authentication-bypass-flaw.html',\n",
       " 'https://thehackernews.com/2020/11/baidus-android-apps-caught-collecting.html',\n",
       " 'https://thehackernews.com/2020/11/stantinko-botnet-now-targeting-linux.html',\n",
       " 'https://thehackernews.com/2020/11/critical-unpatched-vmware-flaw-affects.html',\n",
       " 'https://thehackernews.com/2020/11/why-replace-traditional-web-application.html',\n",
       " 'https://thehackernews.com/2020/11/facebook-messenger-bug-lets-hackers.html']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_url_list = []\n",
    "article_url_list.extend([\n",
    "    article.find('a', class_='story-link')['href'] for article in\n",
    "    soup.find_all('div', class_='body-post clear')\n",
    "])\n",
    "print('Found {} articles:'.format(len(article_url_list)))\n",
    "article_url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this website is structured means that the home page contains links to only a small amount of articles, however this is not enough for our requirements.\n",
    "\n",
    "Additional articles can be found by clicking a button near the bottom of the home page. The following cells show the programme cycling through successive pages, grabbing article URLs, and appending them to the URL list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://thehackernews.com/search?updated-max=2020-11-20T00:31:00-08:00&max-results=8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_page_url = soup.find('a', class_='blog-pager-older-link-mobile')['href']\n",
    "next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(article_url_list) < 300:\n",
    "    response = requests.get(next_page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    article_url_list.extend([\n",
    "        article.find('a', class_='story-link')['href'] for article in\n",
    "        soup.find_all('div', class_='body-post clear')\n",
    "    ])\n",
    "    next_page_url = soup.find('a', class_='blog-pager-older-link-mobile')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 304 article URLs.\n"
     ]
    }
   ],
   "source": [
    "print(\"Found {} article URLs.\".format(len(article_url_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping article data and metadata\n",
    "\n",
    "For each URL, the corresponding article page is loaded. Pieces of data are scraped. Finally, the data is appended to a *pandas DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for url in article_url_list:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h1', class_='story-title').text\n",
    "    date_published = soup.find('meta', {'itemprop': 'datePublished'})['content']\n",
    "    date_modified = soup.find('meta', {'itemprop': 'dateModified'})['content']\n",
    "    author = soup.find('a', {'rel': 'author'}).text\n",
    "    \n",
    "    body_container = soup.find('div', class_='articlebody clear cf')\n",
    "    # In some articles, the body lives inside another div element.\n",
    "    if body_container.find('div', {'dir': 'ltr'}):\n",
    "        body_container = body_container.find('div', {'dir': 'ltr'})\n",
    "    # We need to throw away any divs because they contain junk such as adverts.\n",
    "    for div in body_container.find_all('div'):\n",
    "        div.decompose()\n",
    "    # The article text is left, either in p elements or between line breaks (br)\n",
    "    body = re.sub(pattern=r'\\n+', repl='\\n', string=body_container.text)\n",
    "    \n",
    "    df = df.append({\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'date_published': date_published,\n",
    "        'date_modified': date_modified,\n",
    "        'author': author,\n",
    "        'body': body.strip(),\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the first 5 rows of scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>date_modified</th>\n",
       "      <th>date_published</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Hacker News</td>\n",
       "      <td>Many of us here would love to turn hacking int...</td>\n",
       "      <td>2020-11-26T17:43:03Z</td>\n",
       "      <td>2020-11-25T22:53:00-08:00</td>\n",
       "      <td>Become a White Hat Hacker â€” Get 10 Top-Rated C...</td>\n",
       "      <td>https://thehackernews.com/2020/11/become-white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ravie Lakshmanan</td>\n",
       "      <td>Three Nigerian citizens suspected of being mem...</td>\n",
       "      <td>2020-11-26T06:22:23Z</td>\n",
       "      <td>2020-11-25T22:17:00-08:00</td>\n",
       "      <td>Interpol Arrests 3 Nigerian BEC Scammers For T...</td>\n",
       "      <td>https://thehackernews.com/2020/11/interpol-arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ravie Lakshmanan</td>\n",
       "      <td>cPanel, a provider of popular administrative t...</td>\n",
       "      <td>2020-11-25T07:14:18Z</td>\n",
       "      <td>2020-11-24T23:14:00-08:00</td>\n",
       "      <td>2-Factor Authentication Bypass Flaw Reported i...</td>\n",
       "      <td>https://thehackernews.com/2020/11/2-factor-aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Ravie Lakshmanan</td>\n",
       "      <td>Two popular Android apps from Chinese tech gia...</td>\n",
       "      <td>2020-11-26T06:57:12Z</td>\n",
       "      <td>2020-11-24T22:36:00-08:00</td>\n",
       "      <td>China's Baidu Android Apps Caught Collecting S...</td>\n",
       "      <td>https://thehackernews.com/2020/11/baidus-andro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ravie Lakshmanan</td>\n",
       "      <td>An adware and coin-miner botnet targeting Russ...</td>\n",
       "      <td>2020-11-24T14:56:39Z</td>\n",
       "      <td>2020-11-24T06:56:00-08:00</td>\n",
       "      <td>Stantinko Botnet Now Targeting Linux Servers t...</td>\n",
       "      <td>https://thehackernews.com/2020/11/stantinko-bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               body  \\\n",
       "0   The Hacker News  Many of us here would love to turn hacking int...   \n",
       "1  Ravie Lakshmanan  Three Nigerian citizens suspected of being mem...   \n",
       "2  Ravie Lakshmanan  cPanel, a provider of popular administrative t...   \n",
       "3  Ravie Lakshmanan  Two popular Android apps from Chinese tech gia...   \n",
       "4  Ravie Lakshmanan  An adware and coin-miner botnet targeting Russ...   \n",
       "\n",
       "          date_modified             date_published  \\\n",
       "0  2020-11-26T17:43:03Z  2020-11-25T22:53:00-08:00   \n",
       "1  2020-11-26T06:22:23Z  2020-11-25T22:17:00-08:00   \n",
       "2  2020-11-25T07:14:18Z  2020-11-24T23:14:00-08:00   \n",
       "3  2020-11-26T06:57:12Z  2020-11-24T22:36:00-08:00   \n",
       "4  2020-11-24T14:56:39Z  2020-11-24T06:56:00-08:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Become a White Hat Hacker â€” Get 10 Top-Rated C...   \n",
       "1  Interpol Arrests 3 Nigerian BEC Scammers For T...   \n",
       "2  2-Factor Authentication Bypass Flaw Reported i...   \n",
       "3  China's Baidu Android Apps Caught Collecting S...   \n",
       "4  Stantinko Botnet Now Targeting Linux Servers t...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://thehackernews.com/2020/11/become-white...  \n",
       "1  https://thehackernews.com/2020/11/interpol-arr...  \n",
       "2  https://thehackernews.com/2020/11/2-factor-aut...  \n",
       "3  https://thehackernews.com/2020/11/baidus-andro...  \n",
       "4  https://thehackernews.com/2020/11/stantinko-bo...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, saving the scraped data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hacker_news_articles.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
